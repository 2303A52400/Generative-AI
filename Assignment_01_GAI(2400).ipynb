{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3iurdhbeyb2QboGMLtMlW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52400/Generative-AI/blob/main/Assignment_01_GAI(2400).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D Vidhu Sree\n",
        "2303A52400"
      ],
      "metadata": {
        "id": "edBCvtbF6W5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries"
      ],
      "metadata": {
        "id": "wDkL4q9V6pdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "Y_actual = np.array([20,30,40,50,60])\n",
        "Y_pred = np.array([20.5,30.3,40.2,50.6,60.7])\n",
        "mae = np.mean(np.abs(Y_actual - Y_pred))\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "mse = np.mean((Y_actual - Y_pred) ** 2)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIrpQX-56m4Z",
        "outputId": "0270609a-9ef9-49bc-cc60-73fe987c56fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries"
      ],
      "metadata": {
        "id": "MyANvOd17Wd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define actual and predicted matrices\n",
        "actual = np.array([\n",
        "    [0, 0, 1, 1, 2, 0],\n",
        "    [0, 0, 1, 0, 2, 0],\n",
        "    [0, 1, 1, 2, 2, 1],\n",
        "    [0, 2, 1, 0, 2, 2],\n",
        "    [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "\n",
        "predicted = np.array([\n",
        "    [0, 0, 1, 1, 2, 0],\n",
        "    [0, 0, 1, 0, 2, 0],\n",
        "    [0, 1, 1, 2, 2, 1],\n",
        "    [0, 2, 1, 0, 2, 2],\n",
        "    [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "\n",
        "# Flatten the matrices for computation\n",
        "actual_flat = actual.flatten()\n",
        "predicted_flat = predicted.flatten()\n",
        "\n",
        "# Compute accuracy manually\n",
        "accuracy_manual = np.sum(actual_flat == predicted_flat) / len(actual_flat)\n",
        "\n",
        "# Initialize lists for precision, recall, and F1-score\n",
        "unique_classes = np.unique(actual_flat)\n",
        "precision_manual = []\n",
        "recall_manual = []\n",
        "f1_manual = []\n",
        "\n",
        "# Compute precision, recall, and F1-score manually\n",
        "for cls in unique_classes:\n",
        "    tp = np.sum((actual_flat == cls) & (predicted_flat == cls))\n",
        "    fp = np.sum((actual_flat != cls) & (predicted_flat == cls))\n",
        "    fn = np.sum((actual_flat == cls) & (predicted_flat != cls))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    precision_manual.append(precision)\n",
        "    recall_manual.append(recall)\n",
        "    f1_manual.append(f1)\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score using sklearn\n",
        "accuracy_sklearn = accuracy_score(actual_flat, predicted_flat)\n",
        "precision_sklearn = precision_score(actual_flat, predicted_flat, average=None, zero_division=0)\n",
        "recall_sklearn = recall_score(actual_flat, predicted_flat, average=None, zero_division=0)\n",
        "f1_sklearn = f1_score(actual_flat, predicted_flat, average=None, zero_division=0)\n",
        "\n",
        "# Print results\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_manual:.4f}\")\n",
        "print(f\"Precision: {precision_manual}\")\n",
        "print(f\"Recall: {recall_manual}\")\n",
        "print(f\"F1-Score: {f1_manual}\")\n",
        "\n",
        "print(\"\\nUsing sklearn:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn:.4f}\")\n",
        "print(f\"Precision: {precision_sklearn}\")\n",
        "print(f\"Recall: {recall_sklearn}\")\n",
        "print(f\"F1-Score: {f1_sklearn}\")\n",
        "\n",
        "# Compare manual and sklearn results\n",
        "if (np.isclose(accuracy_manual, accuracy_sklearn) and\n",
        "    np.allclose(precision_manual, precision_sklearn) and\n",
        "    np.allclose(recall_manual, recall_sklearn) and\n",
        "    np.allclose(f1_manual, f1_sklearn)):\n",
        "    print(\"\\nManual calculations match the library results!\")\n",
        "else:\n",
        "    print(\"\\nThere is a discrepancy between manual calculations and library results.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihMWv8dky3Bx",
        "outputId": "277f636f-b5ea-4a23-f71a-4ea17a506b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Accuracy: 1.0000\n",
            "Precision: [1.0, 1.0, 1.0]\n",
            "Recall: [1.0, 1.0, 1.0]\n",
            "F1-Score: [1.0, 1.0, 1.0]\n",
            "\n",
            "Using sklearn:\n",
            "Accuracy: 1.0000\n",
            "Precision: [1. 1. 1.]\n",
            "Recall: [1. 1. 1.]\n",
            "F1-Score: [1. 1. 1.]\n",
            "\n",
            "Manual calculations match the library results!\n"
          ]
        }
      ]
    }
  ]
}